import gradio as gr
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import os
import tempfile

# --- –ï–¢–ê–ü 1: –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø (Loading) ---
def load_document(file_path):
    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—É: {file_path}")
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    return documents

# --- –ï–¢–ê–ü 2: –†–û–ó–ë–ò–í–ö–ê (Splitting) ---
def split_document(documents):
    print("–†–æ–∑–±–∏–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—É –Ω–∞ —á–∞–Ω–∫–∏...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    print(f"–°—Ç–≤–æ—Ä–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫—ñ–≤.")
    return chunks

# --- –ï–¢–ê–ü 3: –í–ï–ö–¢–û–†–ò–ó–ê–¶–Ü–Ø –Ü –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø (Embedding & Storing) ---
def get_vector_store(chunks):
    print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞...")
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model_kwargs = {'device': 'cpu'}
    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω–µ —Å—Ö–æ–≤–∏—â–µ —É —Ç–∏–º—á–∞—Å–æ–≤—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    persist_directory = tempfile.mkdtemp()
    vector_store = Chroma.from_documents(
        documents=chunks, 
        embedding=embeddings,
        persist_directory=persist_directory
    )
    print("–í–µ–∫—Ç–æ—Ä–Ω–µ —Å—Ö–æ–≤–∏—â–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ.")
    return vector_store

# --- –ï–¢–ê–ü 4: –ó–ê–ü–£–°–ö –õ–û–ö–ê–õ–¨–ù–û–á LLM ---
def get_local_llm():
    print("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó LLM...")
    # –®—É–∫–∞—î–º–æ –º–æ–¥–µ–ª—å —É –ø–æ—Ç–æ—á–Ω—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    possible_models = [
      "D:/drone/gemma-3n-E4B-it-Q6_K.gguf",
        "./mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "./llama-2-7b-chat.Q4_K_M.gguf",
        "./vicuna-7b-v1.5.Q4_K_M.gguf"
    ]
    
    model_path = None
    for path in possible_models:
        if os.path.exists(path):
            model_path = path
            break
    
    if not model_path:
        raise FileNotFoundError(
            "–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ GGUF –º–æ–¥–µ–ª—å —Ç–∞ –ø–æ–∫–ª–∞–¥—ñ—Ç—å —ó—ó –≤ –ø–∞–ø–∫—É –∑—ñ —Å–∫—Ä–∏–ø—Ç–æ–º.\n"
            "–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ:\n"
            "- mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
            "- llama-2-7b-chat.Q4_K_M.gguf\n"
            "–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–∂–Ω–∞ –∑: https://huggingface.co/TheBloke"
        )
        
    llm = LlamaCpp(
        model_path=model_path,
        n_gpu_layers=0, 
        n_batch=512,
        n_ctx=4096,
        f16_kv=True,
        verbose=True,
        temperature=0.3,  
        max_tokens=1024,  
        repeat_penalty=1.1,  
        stop=["<end_of_turn>", "</s>", "<|endoftext|>"]  
    )
    print(f"–õ–æ–∫–∞–ª—å–Ω—É LLM –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {model_path}")
    return llm

# --- –ï–¢–ê–ü 5: –ü–û–®–£–ö –¢–ê –ì–ï–ù–ï–†–ê–¶–Ü–Ø (Retrieval & Generation) ---
def create_rag_chain(vector_store, llm):
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    
    template = """<start_of_turn>user
–¢–∏ - –∫–æ—Ä–∏—Å–Ω–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–ü–ò–¢–ê–ù–ù–Ø:
{question}

–Ü–ù–°–¢–†–£–ö–¶–Ü–á:
- –ù–∞–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—É —Ç–∞ –ø–æ–≤–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, —Å–∫–∞–∂–∏ —Ü–µ —á–µ—Å–Ω–æ
- –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —Ä–æ–∑–≥–æ—Ä–Ω—É—Ç–æ —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–æ

–í–Ü–î–ü–û–í–Ü–î–¨:<end_of_turn>
<start_of_turn>model
"""
    
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa_chain

# --- –Ü–ù–¢–ï–†–§–ï–ô–° –ó GRADIO ---
def setup_rag_pipeline(file):
    if file is None:
        return "‚ö†Ô∏è –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ PDF —Ñ–∞–π–ª."
    
    try:
        file_path = file.name
        
        # –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª RAG
        yield "üìñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—É..."
        documents = load_document(file_path)
        
        yield "‚úÇÔ∏è –†–æ–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏..."
        chunks = split_document(documents)
        
        yield "üß† –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞..."
        vector_store = get_vector_store(chunks)
        
        yield "ü§ñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó LLM..."
        llm = get_local_llm()
        
        yield "üîó –°—Ç–≤–æ—Ä–µ–Ω–Ω—è RAG –ª–∞–Ω—Ü—é–∂–∫–∞..."
        qa_chain = create_rag_chain(vector_store, llm)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≥–æ—Ç–æ–≤–∏–π –ª–∞–Ω—Ü—é–∂–æ–∫
        global RAG_CHAIN
        RAG_CHAIN = qa_chain
        
        yield "‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–æ –≤–∞—à–∏—Ö –∑–∞–ø–∏—Ç–∞–Ω—å!"
        
    except Exception as e:
        yield f"‚ùå –ü–æ–º–∏–ª–∫–∞: {str(e)}"

def get_answer(question):
    if not question.strip():
        return "‚ö†Ô∏è –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –ø–∏—Ç–∞–Ω–Ω—è."
    
    if 'RAG_CHAIN' not in globals() or RAG_CHAIN is None:
        return "‚ùå –ü–æ–º–∏–ª–∫–∞: –°–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞. –ë—É–¥—å –ª–∞—Å–∫–∞, —Å–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ PDF."
    
    try:
        print(f"–û–±—Ä–æ–±–∫–∞ –ø–∏—Ç–∞–Ω–Ω—è: {question}")
        result = RAG_CHAIN.invoke({"query": question})
        
        answer = result['result'].strip()
        source_docs = result['source_documents']
        
        print(f"–û—Ç—Ä–∏–º–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {answer}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        if len(answer) < 10 or answer.lower() in ['–±–Ω.', '–Ω/–∞', '–Ω–µ–º–∞—î', '–Ω–µ –∑–Ω–∞—é']:
            # –ü—Ä–æ–±—É—î–º–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è
            reformulated_question = f"–ù–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç—É –¥–µ—Ç–∞–ª—å–Ω–æ –ø–æ—è—Å–Ω–∏: {question}. –ù–∞–¥–∞–π –ø–æ–≤–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É."
            result = RAG_CHAIN.invoke({"query": reformulated_question})
            answer = result['result'].strip()
        
        # –§–æ—Ä–º–∞—Ç—É—î–º–æ –¥–∂–µ—Ä–µ–ª–∞
        sources_text = "\n\nüìö **–î–∂–µ—Ä–µ–ª–∞:**\n"
        for i, doc in enumerate(source_docs, 1):
            page_num = doc.metadata.get('page', '–Ω–µ–≤—ñ–¥–æ–º–∞')
            content_preview = doc.page_content[:150].replace('\n', ' ')
            sources_text += f"\n**{i}.** –°—Ç–æ—Ä—ñ–Ω–∫–∞ {page_num}:\n"
            sources_text += f'"{content_preview}..."\n'
            
        return answer + sources_text
        
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞: {str(e)}")
        return f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –∑–∞–ø–∏—Ç—É: {str(e)}"

# –ì–ª–æ–±–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ –¥–ª—è RAG –ª–∞–Ω—Ü—é–∂–∫–∞
RAG_CHAIN = None

# --- GRADIO –Ü–ù–¢–ï–†–§–ï–ô–° ---
def create_interface():
    with gr.Blocks(theme=gr.themes.Soft(), title="RAG PDF Chat") as demo:
        gr.Markdown("""
        # ü§ñ –ß–∞—Ç –∑ –≤–∞—à–∏–º PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        
        **–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó:**
        1. –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ PDF —Ñ–∞–π–ª
        2. –î–æ—á–µ–∫–∞–π—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –æ–±—Ä–æ–±–∫–∏
        3. –ó–∞–¥–∞–≤–∞–π—Ç–µ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –≤–º—ñ—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç—É
        
        *–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ª–æ–∫–∞–ª—å–Ω—É LLM –º–æ–¥–µ–ª—å - –¥–∞–Ω—ñ –Ω–µ –ø–µ—Ä–µ–¥–∞—é—Ç—å—Å—è –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç!*
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                pdf_upload = gr.File(
                    label="üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –≤–∞—à PDF", 
                    file_types=[".pdf"],
                    height=100
                )
                
                status_text = gr.Textbox(
                    label="üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º–∏", 
                    interactive=False,
                    lines=2
                )
            
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    label="‚ùì –í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è",
                    placeholder="–ù–∞–ø—Ä–∏–∫–ª–∞–¥: –ü—Ä–æ —â–æ —Ü–µ–π –¥–æ–∫—É–º–µ–Ω—Ç?",
                    lines=2
                )
                
                submit_btn = gr.Button("üöÄ –û—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å", variant="primary")
        
        answer_output = gr.Textbox(
            label="üí¨ –í—ñ–¥–ø–æ–≤—ñ–¥—å", 
            lines=15, 
            interactive=False,
            show_copy_button=True
        )
        
        # –ü—Ä–∏–≤'—è–∑—É—î–º–æ –ø–æ–¥—ñ—ó
        pdf_upload.upload(
            fn=setup_rag_pipeline, 
            inputs=pdf_upload, 
            outputs=status_text
        )
        
        submit_btn.click(
            fn=get_answer, 
            inputs=question_input, 
            outputs=answer_output
        )
        
        question_input.submit(
            fn=get_answer, 
            inputs=question_input, 
            outputs=answer_output
        )
        
        # –ü—Ä–∏–∫–ª–∞–¥–∏ –ø–∏—Ç–∞–Ω—å
        gr.Examples(
            examples=[
                "–ü—Ä–æ —â–æ —Ü–µ–π –¥–æ–∫—É–º–µ–Ω—Ç?",
                "–Ø–∫—ñ –æ—Å–Ω–æ–≤–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏?",
                "–•—Ç–æ —î –∞–≤—Ç–æ—Ä–æ–º?",
                "–ö–æ–ª–∏ –±—É–≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–π —Ü–µ–π –¥–æ–∫—É–º–µ–Ω—Ç?",
                "–Ø–∫—ñ –∫–ª—é—á–æ–≤—ñ —Ç–µ—Ä–º–∏–Ω–∏ –∑–≥–∞–¥—É—é—Ç—å—Å—è?"
            ],
            inputs=question_input
        )
    
    return demo

# --- –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ò ---
if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ RAG PDF Chat...")
    print("üìã –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ, —â–æ —É –≤–∞—Å —î:")
    print("   1. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ –≤—Å—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ")
    print("   2. GGUF –º–æ–¥–µ–ª—å —É –ø–∞–ø—Ü—ñ –∑—ñ —Å–∫—Ä–∏–ø—Ç–æ–º")
    print("   3. –î–æ—Å—Ç–∞—Ç–Ω—å–æ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—ó –ø–∞–º'—è—Ç—ñ (–º—ñ–Ω. 8GB)")
    
    demo = create_interface()
    demo.launch(
        share=False,
        server_name="127.0.0.1",
        server_port=7860,
        show_error=True
    )